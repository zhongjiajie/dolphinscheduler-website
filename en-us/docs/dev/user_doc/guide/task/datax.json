{
  "filename": "datax.md",
  "__html": "<h1>DataX</h1>\n<h2>Overview</h2>\n<p>DataX task type for executing DataX programs. For DataX nodes, the worker will execute <code>${DATAX_HOME}/bin/datax.py</code> to analyze the input json file.</p>\n<h2>Create Task</h2>\n<ul>\n<li>Click <code>Project -&gt; Management-Project -&gt; Name-Workflow Definition</code>, and click the <code>Create Workflow</code> button to enter the DAG editing page.</li>\n<li>Drag from the toolbar <img src=\"/img/tasks/icons/datax.png\" width=\"15\"/> task node to canvas.</li>\n</ul>\n<h2>Task Parameter</h2>\n<ul>\n<li><strong>Node name</strong>: The node name in a workflow definition is unique.</li>\n<li><strong>Run flag</strong>: Identifies whether this node schedules normally, if it does not need to execute, select the <code>prohibition execution</code>.</li>\n<li><strong>Descriptive information</strong>: Describe the function of the node.</li>\n<li><strong>Task priority</strong>: When the number of worker threads is insufficient, execute in the order of priority from high to low, and tasks with the same priority will execute in a first-in first-out order.</li>\n<li><strong>Worker grouping</strong>: Assign tasks to the machines of the worker group to execute. If <code>Default</code> is selected, randomly select a worker machine for execution.</li>\n<li><strong>Environment Name</strong>: Configure the environment name in which run the script.</li>\n<li><strong>Times of failed retry attempts</strong>: The number of times the task failed to resubmit.</li>\n<li><strong>Failed retry interval</strong>: The time interval (unit minute) for resubmitting the task after a failed task.</li>\n<li><strong>Delayed execution time</strong>: The time (unit minute) that a task delays in execution.</li>\n<li><strong>Timeout alarm</strong>: Check the timeout alarm and timeout failure. When the task runs exceed the &quot;timeout&quot;, an alarm email will send and the task execution will fail.</li>\n<li><strong>Custom template</strong>: Customize the content of the DataX node's JSON profile when the default DataSource provided does not meet the requirements.</li>\n<li><strong>JSON</strong>: JSON configuration file for DataX synchronization.</li>\n<li><strong>Custom parameters</strong>: SQL task type, and stored procedure is a custom parameter order, to set customized parameter type and data type for the method is the same as the stored procedure task type. The difference is that the custom parameter of the SQL task type replaces the <code>${variable}</code> in the SQL statement.</li>\n<li><strong>Data source</strong>: Select the data source to extract data.</li>\n<li><strong>SQL statement</strong>: The SQL statement used to extract data from the target database, the SQL query column name is automatically parsed when execute the node, and mapped to the target table to synchronize column name. When the column names of the source table and the target table are inconsistent, they can be converted by column alias (as)</li>\n<li><strong>Target library</strong>: Select the target library for data synchronization.</li>\n<li><strong>Pre-SQL</strong>: Pre-SQL executes before the SQL statement (executed by the target database).</li>\n<li><strong>Post-SQL</strong>: Post-SQL executes after the SQL statement (executed by the target database).</li>\n<li><strong>Stream limit (number of bytes)</strong>: Limit the number of bytes for a query.</li>\n<li><strong>Limit flow (number of records)</strong>: Limit the number of records for a query.</li>\n<li><strong>Running memory</strong>: Set the minimum and maximum memory required, which can be set according to the actual production environment.</li>\n<li><strong>Predecessor task</strong>: Selecting a predecessor task for the current task, will set the selected predecessor task as upstream of the current task.</li>\n</ul>\n<h2>Task Example</h2>\n<p>This example demonstrates how to import data from Hive into MySQL.</p>\n<h3>Configure the DataX environment in DolphinScheduler</h3>\n<p>If you are using the DataX task type in a production environment, it is necessary to configure the required environment first. The following is the configuration file: <code>bin/env/dolphinscheduler_env.sh</code>.</p>\n<p><img src=\"/img/tasks/demo/datax_task01.png\" alt=\"datax_task01\"></p>\n<p>After finish the environment configuration, need to restart DolphinScheduler.</p>\n<h3>Configure DataX Task Node</h3>\n<p>As the default DataSource does not contain data read from Hive, require a custom JSON, refer to: <a href=\"https://github.com/alibaba/DataX/blob/master/hdfswriter/doc/hdfswriter.md\">HDFS Writer</a>. Note: Partition directories exist on the HDFS path, when importing data in real world situations, partitioning is recommended to be passed as a parameter, using custom parameters.</p>\n<p>After finish the required JSON file, you can configure the node by following the steps in the diagram below:</p>\n<p><img src=\"/img/tasks/demo/datax_task02.png\" alt=\"datax_task02\"></p>\n<h3>View Execution Result</h3>\n<p><img src=\"/img/tasks/demo/datax_task03.png\" alt=\"datax_task03\"></p>\n<h3>Notice</h3>\n<p>If the default DataSource provided does not meet your needs, you can configure the writer and reader of the DataX according to the actual usage environment in the custom template options, available at <a href=\"https://github.com/alibaba/DataX\">DataX</a>.</p>\n",
  "link": "/dist/en-us/docs/dev/user_doc/guide/task/datax.html",
  "meta": {}
}