{
  "filename": "mlflow.md",
  "__html": "<h1>MLflow Node</h1>\n<h2>Overview</h2>\n<p><a href=\"https://mlflow.org\">MLflow</a> is an excellent open source platform to manage the ML lifecycle, including experimentation,\nreproducibility, deployment, and a central model registry.</p>\n<p>MLflow task plugin used to execute MLflow tasks，Currently contains Mlflow Projects and MLflow Models.（Model Registry will soon be rewarded for support）</p>\n<ul>\n<li>Mlflow Projects: Package data science code in a format to reproduce runs on any platform.</li>\n<li>MLflow Models: Deploy machine learning models in diverse serving environments.</li>\n<li>Model Registry: Store, annotate, discover, and manage models in a central repository.</li>\n</ul>\n<p>The Mlflow plugin currently supports and will support the following:</p>\n<ul>\n<li>[ ] MLflow Projects\n<ul>\n<li>[x] BasicAlgorithm: contains lr, svm, lightgbm, xgboost</li>\n<li>[x] AutoML: AutoML tool，contains autosklean, flaml</li>\n<li>[ ] Custom projects: Support for running your own MLflow projects</li>\n</ul>\n</li>\n<li>[ ] MLflow Models\n<ul>\n<li>[x] MLFLOW: Use <code>MLflow models serve</code> to deploy a model service</li>\n<li>[x] Docker: Run the container after packaging the docker image</li>\n<li>[ ] Docker Compose: Use docker compose to run the container, Will replace the docker run above</li>\n<li>[ ] Seldon core: Use Selcon core to deploy model to k8s cluster</li>\n<li>[ ] k8s: Deploy containers directly to K8S</li>\n<li>[ ] mlflow deployments: Built-in deployment modules, such as built-in deployment to SageMaker, etc</li>\n</ul>\n</li>\n<li>[ ] Model Registry\n<ul>\n<li>[ ] Register Model: Allows artifacts (Including model and related parameters, indicators) to be registered directly into the model center</li>\n</ul>\n</li>\n</ul>\n<h2>Create Task</h2>\n<ul>\n<li>Click <code>Project -&gt; Management-Project -&gt; Name-Workflow Definition</code>, and click the &quot;Create Workflow&quot; button to enter the\nDAG editing page.</li>\n<li>Drag from the toolbar <img src=\"/img/tasks/icons/mlflow.png\" width=\"15\"/> task node to canvas.</li>\n</ul>\n<h2>Task Example</h2>\n<p>First, introduce some general parameters of DolphinScheduler</p>\n<ul>\n<li><strong>Node name</strong>: The node name in a workflow definition is unique.</li>\n<li><strong>Run flag</strong>: Identifies whether this node schedules normally, if it does not need to execute, select\nthe <code>prohibition execution</code>.</li>\n<li><strong>Descriptive information</strong>: Describe the function of the node.</li>\n<li><strong>Task priority</strong>: When the number of worker threads is insufficient, execute in the order of priority from high\nto low, and tasks with the same priority will execute in a first-in first-out order.</li>\n<li><strong>Worker grouping</strong>: Assign tasks to the machines of the worker group to execute. If <code>Default</code> is selected,\nrandomly select a worker machine for execution.</li>\n<li><strong>Environment Name</strong>: Configure the environment name in which run the script.</li>\n<li><strong>Times of failed retry attempts</strong>: The number of times the task failed to resubmit.</li>\n<li><strong>Failed retry interval</strong>: The time interval (unit minute) for resubmitting the task after a failed task.</li>\n<li><strong>Delayed execution time</strong>: The time (unit minute) that a task delays in execution.</li>\n<li><strong>Timeout alarm</strong>: Check the timeout alarm and timeout failure. When the task runs exceed the &quot;timeout&quot;, an alarm\nemail will send and the task execution will fail.</li>\n<li><strong>Predecessor task</strong>: Selecting a predecessor task for the current task, will set the selected predecessor task as\nupstream of the current task.</li>\n</ul>\n<h3>MLflow Projects</h3>\n<h4>BasicAlgorithm</h4>\n<p><img src=\"/img/tasks/demo/mlflow-basic-algorithm.png\" alt=\"mlflow-conda-env\"></p>\n<p><strong>Task Parameter</strong></p>\n<ul>\n<li><strong>mlflow server tracking uri</strong> ：MLflow server uri, default <a href=\"http://localhost:5000\">http://localhost:5000</a>.</li>\n<li><strong>job type</strong> : The type of task to run, currently including the underlying algorithm and AutoML. (User-defined\nMLFlow project task execution will be supported in the near future)</li>\n<li><strong>experiment name</strong> ：The experiment in which the task is running, if none, is created.</li>\n<li><strong>register model</strong> ：Register the model or not. If register is selected, the following parameters are expanded.\n<ul>\n<li><strong>model name</strong> : The registered model name is added to the original model version and registered as\nProduction.</li>\n</ul>\n</li>\n<li><strong>data path</strong> : The absolute path of the file or folder. Ends with .csv for file or contain train.csv and\ntest.csv for folder（In the suggested way, users should build their own test sets for model evaluation）。</li>\n<li><strong>parameters</strong> : Parameter when initializing the algorithm/AutoML model, which can be empty. For example\nparameters <code>&quot;time_budget=30;estimator_list=['lgbm']&quot;</code> for flaml 。The convention will be passed with '; 'shards\neach parameter, using the name before the equal sign as the parameter name, and using the name after the equal\nsign to get the corresponding parameter value through <code>python eval()</code>.\n<ul>\n<li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\">Logistic Regression</a></li>\n<li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html?highlight=svc#sklearn.svm.SVC\">SVM</a></li>\n<li><a href=\"https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier\">lightgbm</a></li>\n<li><a href=\"https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier\">xgboost</a></li>\n</ul>\n</li>\n<li><strong>algorithm</strong> ：The selected algorithm currently supports <code>LR</code>, <code>SVM</code>, <code>LightGBM</code> and <code>XGboost</code> based\non <a href=\"https://scikit-learn.org/\">scikit-learn</a> form.</li>\n<li><strong>Parameter search space</strong> : Parameter search space when running the corresponding algorithm, which can be\nempty. For example, the parameter <code>max_depth=[5, 10];n_estimators=[100, 200]</code> for lightgbm 。The convention\nwill be passed with '; 'shards each parameter, using the name before the equal sign as the parameter name,\nand using the name after the equal sign to get the corresponding parameter value through <code>python eval()</code>.</li>\n</ul>\n<h4>AutoML</h4>\n<p><img src=\"/img/tasks/demo/mlflow-automl.png\" alt=\"mlflow-automl\"></p>\n<p><strong>Task Parameter</strong></p>\n<ul>\n<li><strong>mlflow server tracking uri</strong> ：MLflow server uri, default <a href=\"http://localhost:5000\">http://localhost:5000</a>.</li>\n<li><strong>job type</strong> : The type of task to run, currently including the underlying algorithm and AutoML. (User-defined\nMLFlow project task execution will be supported in the near future)</li>\n<li><strong>experiment name</strong> ：The experiment in which the task is running, if none, is created.</li>\n<li><strong>register model</strong> ：Register the model or not. If register is selected, the following parameters are expanded.\n<ul>\n<li><strong>model name</strong> : The registered model name is added to the original model version and registered as\nProduction.</li>\n</ul>\n</li>\n<li><strong>data path</strong> : The absolute path of the file or folder. Ends with .csv for file or contain train.csv and\ntest.csv for folder（In the suggested way, users should build their own test sets for model evaluation）。</li>\n<li><strong>parameters</strong> : Parameter when initializing the algorithm/AutoML model, which can be empty. For example\nparameters <code>n_estimators=200;learning_rate=0.2</code> for flaml 。The convention will be passed with '; 'shards\neach parameter, using the name before the equal sign as the parameter name, and using the name after the equal\nsign to get the corresponding parameter value through <code>python eval()</code>. The detailed parameter list is as follows:\n<ul>\n<li><a href=\"https://microsoft.github.io/FLAML/docs/reference/automl#automl-objects\">flaml</a></li>\n<li><a href=\"https://automl.github.io/auto-sklearn/master/api.html\">autosklearn</a></li>\n</ul>\n</li>\n<li><strong>AutoML tool</strong> : The AutoML tool used, currently\nsupports <a href=\"https://github.com/automl/auto-sklearn\">autosklearn</a>\nand <a href=\"https://github.com/microsoft/FLAML\">flaml</a></li>\n</ul>\n<h3>MLflow Models</h3>\n<h4>MLFLOW</h4>\n<p><img src=\"/img/tasks/demo/mlflow-models-mlflow.png\" alt=\"mlflow-models-mlflow\"></p>\n<p><strong>Task Parameter</strong></p>\n<ul>\n<li><strong>mlflow server tracking uri</strong> ：MLflow server uri, default <a href=\"http://localhost:5000\">http://localhost:5000</a>.</li>\n<li><strong>model-uri</strong> ：Model-uri of mlflow , support <code>models:/&lt;model_name&gt;/suffix</code> format and <code>runs:/</code> format. See <a href=\"https://mlflow.org/docs/latest/tracking.html#artifact-stores\">https://mlflow.org/docs/latest/tracking.html#artifact-stores</a></li>\n<li><strong>Port</strong> ：The port to listen on</li>\n</ul>\n<h4>Docker</h4>\n<p><img src=\"/img/tasks/demo/mlflow-models-docker.png\" alt=\"mlflow-models-docker\"></p>\n<p><strong>Task Parameter</strong></p>\n<ul>\n<li><strong>mlflow server tracking uri</strong> ：MLflow server uri, default <a href=\"http://localhost:5000\">http://localhost:5000</a>.</li>\n<li><strong>model-uri</strong> ：Model-uri of mlflow , support <code>models:/&lt;model_name&gt;/suffix</code> format and <code>runs:/</code> format. See <a href=\"https://mlflow.org/docs/latest/tracking.html#artifact-stores\">https://mlflow.org/docs/latest/tracking.html#artifact-stores</a></li>\n<li><strong>Port</strong> ：The port to listen on</li>\n</ul>\n<h2>Environment to prepare</h2>\n<h3>Conda env</h3>\n<p>You need to enter the admin account to configure a conda environment variable（Please\ninstall <a href=\"https://docs.continuum.io/anaconda/install/\">anaconda</a>\nor <a href=\"https://docs.conda.io/en/latest/miniconda.html#installing\">miniconda</a> in advance )</p>\n<p><img src=\"/img/tasks/demo/mlflow-conda-env.png\" alt=\"mlflow-conda-env\"></p>\n<p>Note During the configuration task, select the conda environment created above. Otherwise, the program cannot find the\nConda environment.</p>\n<p><img src=\"/img/tasks/demo/mlflow-set-conda-env.png\" alt=\"mlflow-set-conda-env\"></p>\n<h3>Start the mlflow service</h3>\n<p>Make sure you have installed MLflow, using 'PIP Install MLFlow'.</p>\n<p>Create a folder where you want to save your experiments and models and start mlFlow service.</p>\n<pre><code class=\"language-sh\">mkdir mlflow\n<span class=\"hljs-built_in\">cd</span> mlflow\nmlflow server -h 0.0.0.0 -p 5000 --serve-artifacts --backend-store-uri sqlite:///mlflow.db\n</code></pre>\n<p>After running, an MLflow service is started</p>\n<p>After this, you can visit the MLFlow service (<code>http://localhost:5000</code>) page to view the experiments and models.</p>\n<p><img src=\"/img/tasks/demo/mlflow-server.png\" alt=\"mlflow-server\"></p>\n",
  "link": "/dist/en-us/docs/dev/user_doc/guide/task/mlflow.html",
  "meta": {}
}